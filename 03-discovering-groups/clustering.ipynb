{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip3 install feedparser\n",
    "import re\n",
    "import os\n",
    "import feedparser\n",
    "from math import sqrt\n",
    "\n",
    "from scipy import stats\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(html):\n",
    "    # ! Tags are sanitized by default.\n",
    "    txt = re.compile(r'<[^>]+>').sub('', html)\n",
    "\n",
    "    # Split words by all non-alpha characters.\n",
    "    words = re.compile(r'[^A-Z^a-z]+').split(txt)\n",
    "    \n",
    "    # Convert to lowercase.\n",
    "    return [word.lower() for word in words if word != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_counts(url):\n",
    "    # Parse the feed.\n",
    "    title, words = parse_feed(url)\n",
    "    return title, Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_feed(url):\n",
    "    feed = feedparser.parse(url)\n",
    "    title = ''\n",
    "    for field in ['title']:\n",
    "        if field in feed.feed:\n",
    "            title = feed.feed[field]\n",
    "    \n",
    "    # Consolidate the feed entries by title and description.\n",
    "    words = [word for e in feed.entries for word in get_words(f'{e.title} {e.summary}')]\n",
    "    return title, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Signal v. Noise', ['the', 'essential', 'questions', 'to', 'ask'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title, words = parse_feed('http://feeds.feedburner.com/37signals/beMH')\n",
    "title, words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedlist = '''http://feeds.feedburner.com/37signals/beMH\n",
    "http://feeds.feedburner.com/blogspot/bRuz\n",
    "http://feeds.feedburner.com/JohnBattellesSearchblog\n",
    "https://guykawasaki.com/feed/\n",
    "http://blog.outer-court.com/rss.xml\n",
    "https://searchenginewatch.com/tag/rss-feed/feed/\n",
    "http://www.topix.com//rss/news/blogs\n",
    "http://feeds.abcnews.com/abcnews/blotterheadlines\n",
    "https://gigaom.com/feed/\n",
    "http://gizmodo.com/index.xml\n",
    "http://gofugyourself.typepad.com/go_fug_yourself/index.rdf\n",
    "http://googleblog.blogspot.com/rss.xml\n",
    "http://feeds.feedburner.com/GoogleOperatingSystem\n",
    "http://headrush.typepad.com/creating_passionate_users/index.rdf\n",
    "http://feeds.feedburner.com/pjmedia/instapundit\n",
    "https://blog.zawodny.com/feed/\n",
    "http://joi.ito.com/index.rdf\n",
    "http://feeds.feedburner.com/Mashable\n",
    "http://michellemalkin.com/feed/\n",
    "http://moblogsmoproblems.blogspot.com/rss.xml\n",
    "http://newsbusters.org/node/feed\n",
    "http://feeds.feedburner.com/paulstamatiou\n",
    "http://feeds.feedburner.com/powerlineblog/livefeed\n",
    "http://radar.oreilly.com/index.rdf\n",
    "http://scienceblogs.com/pharyngula/feed/\n",
    "http://scobleizer.wordpress.com/feed/\n",
    "http://sethgodin.typepad.com/seths_blog/index.rdf\n",
    "http://rss.slashdot.org/Slashdot/slashdot\n",
    "http://thinkprogress.org/feed/\n",
    "http://feeds.feedburner.com/andrewsullivan/rApM\n",
    "http://wilwheaton.typepad.com/wwdnbackup/index.rdf\n",
    "http://www.43folders.com/feed/\n",
    "http://www.456bereastreet.com/feed.xml\n",
    "http://www.autoblog.com/rss.xml\n",
    "http://www.bloggersblog.com/rss.xml\n",
    "http://www.blogmaverick.com/rss.xml\n",
    "http://www.boingboing.net/index.rdf\n",
    "http://www.buzzmachine.com/index.xml\n",
    "http://www.captainsquartersblog.com/mt/index.rdf\n",
    "http://feeds.coolhunting.com/ch\n",
    "http://feeds.copyblogger.com/Copyblogger\n",
    "http://feeds.feedburner.com/crooksandliars/YaCP\n",
    "http://feeds.dailykos.com/dailykos/index.xml\n",
    "http://www.deadspin.com/index.xml\n",
    "http://www.huffingtonpost.com/feeds/verticals/technology/index.xml\n",
    "http://www.engadget.com/rss.xml\n",
    "https://www.gapingvoid.com/feed/\n",
    "http://www.gothamist.com/index.rdf\n",
    "http://www.huffingtonpost.com/raw_feed_index.rdf\n",
    "http://www.hyperorg.com/blogger/index.rdf\n",
    "http://www.joelonsoftware.com/rss.xml\n",
    "http://www.kotaku.com/index.xml\n",
    "http://feeds.kottke.org/main\n",
    "http://www.lifehack.org/feed/\n",
    "http://www.lifehacker.com/index.xml\n",
    "http://site2.littlegreenfootballs.com/feed\n",
    "http://makezine.com/feed/\n",
    "http://www.mattcutts.com/blog/feed/\n",
    "http://xml.metafilter.com/rss.xml\n",
    "http://www.mezzoblue.com/rss/index.xml\n",
    "http://www.neilgaiman.com/journal/feed/rss.xml\n",
    "http://www.oilman.ca/feed/\n",
    "http://www.perezhilton.com/index.xml\n",
    "http://www.plasticbag.org/index.rdf\n",
    "http://www.powazek.com/rss.xml\n",
    "http://www.problogger.net/feed/\n",
    "http://feeds.feedburner.com/QuickOnlineTips\n",
    "http://readwrite.com/feed/\n",
    "http://www.schneier.com/blog/index.rdf\n",
    "http://scienceblogs.com/feed/\n",
    "http://www.seroundtable.com/index.rdf\n",
    "http://www.shoemoney.com/feed/\n",
    "http://www.sifry.com/alerts?format=RSS\n",
    "http://simplebits.com/feed.xml\n",
    "http://feeds.feedburner.com/Spikedhumor\n",
    "http://www.stevepavlina.com/blog/feed\n",
    "https://talkingpointsmemo.com/feed/all\n",
    "http://www.tbray.org/ongoing/ongoing.rss\n",
    "http://feeds.feedburner.com/TechCrunch\n",
    "http://www.techdirt.com/techdirt_rss.xml\n",
    "http://www.techeblog.com/elephant/?mode=atom\n",
    "http://www.thesuperficial.com/feed\n",
    "http://www.tmz.com/rss.xml\n",
    "https://www.treehugger.com/feeds/latest/\n",
    "http://feeds.gawker.com/gizmodo/full\n",
    "http://we-make-money-not-art.com/feed/\n",
    "http://www.wired.com/rss/index.xml\n",
    "https://wonkette.com/feed'''.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing http://feeds.feedburner.com/37signals/beMH\n",
      "parsing http://feeds.feedburner.com/blogspot/bRuz\n",
      "parsing http://feeds.feedburner.com/JohnBattellesSearchblog\n",
      "parsing https://guykawasaki.com/feed/\n",
      "parsing http://blog.outer-court.com/rss.xml\n",
      "parsing https://searchenginewatch.com/tag/rss-feed/feed/\n",
      "parsing http://www.topix.com//rss/news/blogs\n",
      "parsing http://feeds.abcnews.com/abcnews/blotterheadlines\n",
      "parsing https://gigaom.com/feed/\n",
      "parsing http://gizmodo.com/index.xml\n",
      "parsing http://gofugyourself.typepad.com/go_fug_yourself/index.rdf\n",
      "parsing http://googleblog.blogspot.com/rss.xml\n",
      "parsing http://feeds.feedburner.com/GoogleOperatingSystem\n",
      "parsing http://headrush.typepad.com/creating_passionate_users/index.rdf\n",
      "parsing http://feeds.feedburner.com/pjmedia/instapundit\n",
      "parsing https://blog.zawodny.com/feed/\n",
      "https://blog.zawodny.com/feed/ generator raised StopIteration\n",
      "parsing http://joi.ito.com/index.rdf\n",
      "parsing http://feeds.feedburner.com/Mashable\n",
      "parsing http://michellemalkin.com/feed/\n",
      "parsing http://moblogsmoproblems.blogspot.com/rss.xml\n",
      "parsing http://newsbusters.org/node/feed\n",
      "parsing http://feeds.feedburner.com/paulstamatiou\n",
      "parsing http://feeds.feedburner.com/powerlineblog/livefeed\n",
      "parsing http://radar.oreilly.com/index.rdf\n",
      "parsing http://scienceblogs.com/pharyngula/feed/\n",
      "parsing http://scobleizer.wordpress.com/feed/\n",
      "parsing http://sethgodin.typepad.com/seths_blog/index.rdf\n",
      "parsing http://rss.slashdot.org/Slashdot/slashdot\n",
      "parsing http://thinkprogress.org/feed/\n",
      "parsing http://feeds.feedburner.com/andrewsullivan/rApM\n",
      "parsing http://wilwheaton.typepad.com/wwdnbackup/index.rdf\n",
      "parsing http://www.43folders.com/feed/\n",
      "parsing http://www.456bereastreet.com/feed.xml\n",
      "parsing http://www.autoblog.com/rss.xml\n",
      "parsing http://www.bloggersblog.com/rss.xml\n",
      "parsing http://www.blogmaverick.com/rss.xml\n",
      "parsing http://www.boingboing.net/index.rdf\n",
      "parsing http://www.buzzmachine.com/index.xml\n",
      "parsing http://www.captainsquartersblog.com/mt/index.rdf\n",
      "parsing http://feeds.coolhunting.com/ch\n",
      "parsing http://feeds.copyblogger.com/Copyblogger\n",
      "parsing http://feeds.feedburner.com/crooksandliars/YaCP\n",
      "parsing http://feeds.dailykos.com/dailykos/index.xml\n",
      "parsing http://www.deadspin.com/index.xml\n",
      "parsing http://www.huffingtonpost.com/feeds/verticals/technology/index.xml\n",
      "parsing http://www.engadget.com/rss.xml\n",
      "parsing https://www.gapingvoid.com/feed/\n",
      "parsing http://www.gothamist.com/index.rdf\n",
      "parsing http://www.huffingtonpost.com/raw_feed_index.rdf\n",
      "parsing http://www.hyperorg.com/blogger/index.rdf\n",
      "parsing http://www.joelonsoftware.com/rss.xml\n",
      "parsing http://www.kotaku.com/index.xml\n",
      "parsing http://feeds.kottke.org/main\n",
      "parsing http://www.lifehack.org/feed/\n",
      "parsing http://www.lifehacker.com/index.xml\n",
      "parsing http://site2.littlegreenfootballs.com/feed\n",
      "parsing http://makezine.com/feed/\n",
      "parsing http://www.mattcutts.com/blog/feed/\n",
      "parsing http://xml.metafilter.com/rss.xml\n",
      "parsing http://www.mezzoblue.com/rss/index.xml\n",
      "parsing http://www.neilgaiman.com/journal/feed/rss.xml\n",
      "parsing http://www.oilman.ca/feed/\n",
      "parsing http://www.perezhilton.com/index.xml\n",
      "parsing http://www.plasticbag.org/index.rdf\n",
      "parsing http://www.powazek.com/rss.xml\n",
      "parsing http://www.problogger.net/feed/\n",
      "parsing http://feeds.feedburner.com/QuickOnlineTips\n",
      "parsing http://readwrite.com/feed/\n",
      "parsing http://www.schneier.com/blog/index.rdf\n",
      "parsing http://scienceblogs.com/feed/\n",
      "parsing http://www.seroundtable.com/index.rdf\n",
      "parsing http://www.shoemoney.com/feed/\n",
      "parsing http://www.sifry.com/alerts?format=RSS\n",
      "parsing http://simplebits.com/feed.xml\n",
      "parsing http://feeds.feedburner.com/Spikedhumor\n",
      "parsing http://www.stevepavlina.com/blog/feed\n",
      "parsing https://talkingpointsmemo.com/feed/all\n",
      "parsing http://www.tbray.org/ongoing/ongoing.rss\n",
      "parsing http://feeds.feedburner.com/TechCrunch\n",
      "parsing http://www.techdirt.com/techdirt_rss.xml\n",
      "parsing http://www.techeblog.com/elephant/?mode=atom\n",
      "parsing http://www.thesuperficial.com/feed\n",
      "parsing http://www.tmz.com/rss.xml\n",
      "parsing https://www.treehugger.com/feeds/latest/\n",
      "parsing http://feeds.gawker.com/gizmodo/full\n",
      "parsing http://we-make-money-not-art.com/feed/\n",
      "parsing http://www.wired.com/rss/index.xml\n",
      "parsing https://wonkette.com/feed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if not os.path.exists('feed.pickle'):\n",
    "# The number of blogs each word appeared.\n",
    "appeared_counts = defaultdict(int)\n",
    "\n",
    "# The word count for each blog.\n",
    "word_counts = defaultdict(Counter)\n",
    "\n",
    "for url in feedlist:\n",
    "    try:\n",
    "        print(f'parsing {url}')\n",
    "        title, wc = get_word_counts(url)\n",
    "        word_counts[title] = wc\n",
    "        for word, count in wc.items():\n",
    "            if count > 1:\n",
    "                appeared_counts[word] += 1\n",
    "    except Exception as e:\n",
    "        print(url, e)\n",
    "True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# try:\n",
    "#     with open('feed.pickle', 'rb') as f:\n",
    "#         data = pickle.load(f)\n",
    "#         print('loaded', len(data))\n",
    "#         word_counts = data['word_counts']\n",
    "#         appeared_counts = data['appeared_counts']\n",
    "# except Exception as e:\n",
    "#     with open('feed.pickle', 'wb+') as f:\n",
    "#         wc = {}\n",
    "#         for key, value in word_counts.items():\n",
    "#             wc[key] = value\n",
    "#         data = {'appeared_counts': appeared_counts,\n",
    "#                 'word_counts': wc}\n",
    "#         pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 69), ('questions', 16), ('to', 69), ('ask', 7), ('as', 58)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(appeared_counts.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Signal v. Noise',\n",
       " [('the', 40), ('a', 25), ('of', 14), ('and', 14), ('to', 12)])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title, counter = list(word_counts.items())[0]\n",
    "title, counter.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['questions', 'better', 'best', 'some', 'point']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlist = []\n",
    "for w, bc in appeared_counts.items():\n",
    "    frac = float(bc) / len(feedlist)\n",
    "    # Skip the ones that are less than 10% and more than 50%.\n",
    "    if frac > 0.1 and frac < 0.5: wordlist.append(w)\n",
    "wordlist[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('blogdata.txt', 'w') as f:\n",
    "    # First line is the header.\n",
    "    f.write('Blog')\n",
    "    for word in wordlist: f.write(f'\\t{word}')\n",
    "    f.write('\\n')\n",
    "    \n",
    "    # Subsequent lines are the body.\n",
    "    for blog, wc in word_counts.items():\n",
    "        if blog.strip() == '':\n",
    "            continue\n",
    "        f.write(blog)\n",
    "        for word in wordlist:\n",
    "            if word in wc: f.write(f'\\t{wc[word]}')\n",
    "            else: f.write(f'\\t{0}')\n",
    "        f.write('\\n')\n",
    "True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    lines = [line for line in open(filename)]\n",
    "    \n",
    "    # First line is the column title.\n",
    "    cols = lines[0].strip().split('\\t')[1:]\n",
    "    rows, data = [], []\n",
    "    \n",
    "    for line in lines[1:]:\n",
    "        p = line.strip().split('\\t')\n",
    "        title = p[0]\n",
    "        if title.strip() == '': continue\n",
    "        \n",
    "        # First column in each row is the row name.\n",
    "        rows.append(p[0])\n",
    "        \n",
    "        # The data for this row is the remainder of the row.\n",
    "        data.append([float(x) for x in p[1:]])\n",
    "    return rows, cols, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols, data = read_file('blogdata.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearsonr(v1, v2):\n",
    "    \"\"\"Pearson correlation determines how similar two items are.\"\"\"\n",
    "    \n",
    "    n = min(len(v1), len(v2))\n",
    "    v1, v2 = v1[:n], v2[:n]\n",
    "    # Simple sums.\n",
    "    sum1 = sum(v1)\n",
    "    sum2 = sum(v2)\n",
    "    \n",
    "    \n",
    "    # Sum of squares.\n",
    "    sum1_square = sum(pow(v, 2) for v in v1)\n",
    "    sum2_square = sum(pow(v, 2) for v in v2)\n",
    "    \n",
    "    # Sum of products.\n",
    "    sum_products = sum([v1[i] * v2[i] for i in range(n)])\n",
    "    \n",
    "    # Calculate the pearson score.\n",
    "    num = sum_products - (sum1 * sum2 / n)\n",
    "    den = sqrt((sum1_square - pow(sum1, 2) / n) * (sum2_square - pow(sum2, 2) / n))\n",
    "    if den == 0: return 0\n",
    "    return num / den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9798637100971993, (0.9798637100971993, 0.12797239868771323))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v, w = [1,2,4], [1,2,8]\n",
    "pearsonr(v, w), stats.pearsonr(v, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bicluster:\n",
    "    def __init__(self, vec, left = None, right = None, distance = 0.0, id = None):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.vec = vec\n",
    "        self.distance = distance\n",
    "        self.id = id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hcluster(rows, distance = pearsonr):\n",
    "    # Distances is the cache of the distance calculation.\n",
    "    distances = {}\n",
    "    current_cluster_id = -1\n",
    "    \n",
    "    # Clusters are initially just the rows.\n",
    "    cluster = [bicluster(rows[i], id = i) for i in range(len(rows))]\n",
    "    \n",
    "    while len(cluster) > 1:\n",
    "        lowest_pair = (0, 1)\n",
    "        closest = distance(cluster[0].vec, cluster[1].vec)\n",
    "        \n",
    "        # Loop through every pair looking for the smallest distance.\n",
    "        for i in range(len(cluster)):\n",
    "            for j in range(i + 1, len(cluster)):\n",
    "                key = (cluster[i].id, cluster[j].id)\n",
    "                if key not in distances:\n",
    "                    distances[key] = distance(cluster[i].vec, cluster[j].vec)\n",
    "                d = distances[key]\n",
    "                if d < closest:\n",
    "                    closest = d\n",
    "                    lowest_pair = (i, j)\n",
    "        (x, y) = lowest_pair\n",
    "\n",
    "        # Calculate the average of the two clusters.\n",
    "        merge_vectors = [(cluster[x].vec[i] + cluster[y].vec[i])/2.0 \n",
    "                         for i in range(len(cluster[0].vec))]\n",
    "        \n",
    "        # Create the new cluster.\n",
    "        new_cluster = bicluster(merge_vectors, \n",
    "                                left=cluster[x],\n",
    "                                right=cluster[y],\n",
    "                                distance=closest,\n",
    "                                id=current_cluster_id)\n",
    "        \n",
    "        # Cluster ids that weren't in the original set are negative.\n",
    "        current_cluster_id -= 1\n",
    "        del cluster[y]\n",
    "        del cluster[x]\n",
    "        cluster.append(new_cluster)\n",
    "    return cluster[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "blognames, words, data = read_file('blogdata.txt')\n",
    "cluster = hcluster(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cluster(cluster, labels = None, n = 0):\n",
    "    # Indent to make a hierachy layout.\n",
    "    for i in range(n): print(' ', end='')\n",
    "    if cluster.id < 0:\n",
    "        # Negative id means that this is branch.\n",
    "        print('-')\n",
    "    else:\n",
    "        # Positive id means that this is an endpoint.\n",
    "        if labels == None: print(cluster.id),\n",
    "        else: print(labels[cluster.id]),\n",
    "    \n",
    "    # Now print the right and left branches.\n",
    "    if cluster.left != None: print_cluster(cluster.left, labels = labels, n = n + 1)\n",
    "    if cluster.right != None: print_cluster(cluster.right, labels = labels, n= n + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      " -\n",
      "  -\n",
      "   -\n",
      "    Sifry's Alerts - David Sifry\n",
      "    -\n",
      "     Instapundit\n",
      "     Schneier on Security\n",
      "   -\n",
      "    -\n",
      "     Lifehacker\n",
      "     -\n",
      "      NB Blog Feed\n",
      "      -\n",
      "       RSS feed â€“ Search Engine Watch\n",
      "       We Make Money Not Art\n",
      "    -\n",
      "     Techdirt.\n",
      "     -\n",
      "      Captain's Quarters\n",
      "      Lifehack - Feed\n",
      "  -\n",
      "   -\n",
      "    Boing Boing\n",
      "    -\n",
      "     Derek Powazek\n",
      "     -\n",
      "      Latest Items from TreeHugger\n",
      "      -\n",
      "       Tech\n",
      "       Make: DIY Projects and Ideas for Makers\n",
      "   -\n",
      "    -\n",
      "     -\n",
      "      Neil Gaiman's Journal\n",
      "      -\n",
      "       ReadWrite\n",
      "       Talking Points Memo\n",
      "     -\n",
      "      The Official Google Blog\n",
      "      -\n",
      "       Kotaku\n",
      "       -\n",
      "        PaulStamatiou.com - Technology, Design and Photography\n",
      "        BuzzMachine\n",
      "    -\n",
      "     -\n",
      "      Guy Kawasaki\n",
      "      -\n",
      "       Quick Online Tips\n",
      "       TMZ.com\n",
      "     -\n",
      "      MichelleMalkin.com\n",
      "      pharyngula\n",
      " -\n",
      "  -\n",
      "   -\n",
      "    -\n",
      "     Google Blogoscoped\n",
      "     Gothamist\n",
      "    -\n",
      "     -\n",
      "      The Write News\n",
      "      ongoing by Tim Bray\n",
      "     -\n",
      "      Gapingvoid\n",
      "      -\n",
      "       Joi Ito's Web\n",
      "       Wired\n",
      "   -\n",
      "    -\n",
      "     The Viral Garden\n",
      "     -\n",
      "      -\n",
      "       Deadspin\n",
      "       -\n",
      "        -\n",
      "         Creating Passionate Users\n",
      "         Little Green Footballs\n",
      "        -\n",
      "         Gigaom\n",
      "         mezzoblue\n",
      "      -\n",
      "       John Battelle's Search Blog\n",
      "       Google Operating System\n",
      "    -\n",
      "     -\n",
      "      SimpleBits\n",
      "      -\n",
      "       Gizmodo\n",
      "       ProBlogger\n",
      "     -\n",
      "      Wonkette\n",
      "      -\n",
      "       Seth Godin's Blog on marketing, tribes and respect\n",
      "       TechCrunch\n",
      "  -\n",
      "   -\n",
      "    -\n",
      "     ShoeMoney\n",
      "     -\n",
      "      Signal v. Noise\n",
      "      -\n",
      "       O'Reilly Radar\n",
      "       ThinkProgress\n",
      "    -\n",
      "     WIL WHEATON dot NET\n",
      "     -\n",
      "      Slashdot\n",
      "      -\n",
      "       Matt Cutts: Gadgets, Google, and SEO\n",
      "       -\n",
      "        Autoblog\n",
      "        Latest from Crooks and Liars\n",
      "   -\n",
      "    ScienceBlogs - Where the world discusses science\n",
      "    -\n",
      "     -\n",
      "      -\n",
      "       Eschaton\n",
      "       ABC News: BLOTTER\n",
      "      -\n",
      "       Joel on Software\n",
      "       -\n",
      "        COOL HUNTING\n",
      "        Search Engine Roundtable\n",
      "     -\n",
      "      -\n",
      "       blog maverick\n",
      "       -\n",
      "        Power LinePower Line\n",
      "        Copyblogger\n",
      "      -\n",
      "       -\n",
      "        The Dish\n",
      "        Engadget RSS Feed\n",
      "       -\n",
      "        Mashable\n",
      "        kottke.org\n"
     ]
    }
   ],
   "source": [
    "print_cluster(cluster, blognames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
