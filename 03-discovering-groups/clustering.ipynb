{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "import re\n",
    "\n",
    "# Returns title and dictionary of word counts for an RSS Feed.\n",
    "def get_word_counts(url):\n",
    "    # Parse the feed.\n",
    "    d = feedparser.parse(url)\n",
    "\n",
    "    # The title of the feed.\n",
    "    title = d.feed.title\n",
    "\n",
    "    wc = {}\n",
    "    \n",
    "    # Loop over all the entries.\n",
    "    for e in d.entries:\n",
    "        if 'summary' in e: summary = e.summary\n",
    "        else: summary = e.description\n",
    "        \n",
    "        # Extract a list of words.\n",
    "        words = get_words(f'{title} {summary}')\n",
    "        for word in words:\n",
    "            wc.setdefault(word, 0)\n",
    "            wc[word] += 1\n",
    "    return title, wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(html):\n",
    "    # Remove all html tags.\n",
    "    txt = re.compile(r'<[^>]+>').sub('', html)\n",
    "    \n",
    "    # Split words by all non-alpha characters.\n",
    "    words = re.compile(r'[^A-Z^a-z]+').split(txt)\n",
    "    \n",
    "    # Convert to lowercase.\n",
    "    return [word.lower() for word in words if word != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedlist = '''http://feeds.feedburner.com/37signals/beMH\n",
    "http://feeds.feedburner.com/blogspot/bRuz\n",
    "http://feeds.feedburner.com/JohnBattellesSearchblog\n",
    "https://guykawasaki.com/feed/\n",
    "http://blog.outer-court.com/rss.xml\n",
    "https://searchenginewatch.com/tag/rss-feed/feed/\n",
    "http://www.topix.com//rss/news/blogs\n",
    "http://feeds.abcnews.com/abcnews/blotterheadlines\n",
    "https://gigaom.com/feed/\n",
    "http://gizmodo.com/index.xml\n",
    "http://gofugyourself.typepad.com/go_fug_yourself/index.rdf\n",
    "http://googleblog.blogspot.com/rss.xml\n",
    "http://feeds.feedburner.com/GoogleOperatingSystem\n",
    "http://headrush.typepad.com/creating_passionate_users/index.rdf\n",
    "http://feeds.feedburner.com/pjmedia/instapundit\n",
    "https://blog.zawodny.com/feed/\n",
    "http://joi.ito.com/index.rdf\n",
    "http://feeds.feedburner.com/Mashable\n",
    "http://michellemalkin.com/feed/\n",
    "http://moblogsmoproblems.blogspot.com/rss.xml\n",
    "http://newsbusters.org/node/feed\n",
    "http://feeds.feedburner.com/paulstamatiou\n",
    "http://feeds.feedburner.com/powerlineblog/livefeed\n",
    "http://radar.oreilly.com/index.rdf\n",
    "http://scienceblogs.com/pharyngula/feed/\n",
    "http://scobleizer.wordpress.com/feed/\n",
    "http://sethgodin.typepad.com/seths_blog/index.rdf\n",
    "http://rss.slashdot.org/Slashdot/slashdot\n",
    "http://thinkprogress.org/feed/\n",
    "http://feeds.feedburner.com/andrewsullivan/rApM\n",
    "http://wilwheaton.typepad.com/wwdnbackup/index.rdf\n",
    "http://www.43folders.com/feed/\n",
    "http://www.456bereastreet.com/feed.xml\n",
    "http://www.autoblog.com/rss.xml\n",
    "http://www.bloggersblog.com/rss.xml\n",
    "http://www.blogmaverick.com/rss.xml\n",
    "http://www.boingboing.net/index.rdf\n",
    "http://www.buzzmachine.com/index.xml\n",
    "http://www.captainsquartersblog.com/mt/index.rdf\n",
    "http://feeds.coolhunting.com/ch\n",
    "http://feeds.copyblogger.com/Copyblogger\n",
    "http://feeds.feedburner.com/crooksandliars/YaCP\n",
    "http://feeds.dailykos.com/dailykos/index.xml\n",
    "http://www.deadspin.com/index.xml\n",
    "http://www.huffingtonpost.com/feeds/verticals/technology/index.xml\n",
    "http://www.engadget.com/rss.xml\n",
    "https://www.gapingvoid.com/feed/\n",
    "http://www.gothamist.com/index.rdf\n",
    "http://www.huffingtonpost.com/raw_feed_index.rdf\n",
    "http://www.hyperorg.com/blogger/index.rdf\n",
    "http://www.joelonsoftware.com/rss.xml\n",
    "http://www.kotaku.com/index.xml\n",
    "http://feeds.kottke.org/main\n",
    "http://www.lifehack.org/feed/\n",
    "http://www.lifehacker.com/index.xml\n",
    "http://site2.littlegreenfootballs.com/feed\n",
    "http://makezine.com/feed/\n",
    "http://www.mattcutts.com/blog/feed/\n",
    "http://xml.metafilter.com/rss.xml\n",
    "http://www.mezzoblue.com/rss/index.xml\n",
    "http://www.neilgaiman.com/journal/feed/rss.xml\n",
    "http://www.oilman.ca/feed/\n",
    "http://www.perezhilton.com/index.xml\n",
    "http://www.plasticbag.org/index.rdf\n",
    "http://www.powazek.com/rss.xml\n",
    "http://www.problogger.net/feed/\n",
    "http://feeds.feedburner.com/QuickOnlineTips\n",
    "http://readwrite.com/feed/\n",
    "http://www.schneier.com/blog/index.rdf\n",
    "http://scienceblogs.com/feed/\n",
    "http://www.seroundtable.com/index.rdf\n",
    "http://www.shoemoney.com/feed/\n",
    "http://www.sifry.com/alerts?format=RSS\n",
    "http://simplebits.com/feed.xml\n",
    "http://feeds.feedburner.com/Spikedhumor\n",
    "http://www.stevepavlina.com/blog/feed\n",
    "https://talkingpointsmemo.com/feed/all\n",
    "http://www.tbray.org/ongoing/ongoing.rss\n",
    "http://feeds.feedburner.com/TechCrunch\n",
    "http://www.techdirt.com/techdirt_rss.xml\n",
    "http://www.techeblog.com/elephant/?mode=atom\n",
    "http://www.thesuperficial.com/feed\n",
    "http://www.tmz.com/rss.xml\n",
    "https://www.treehugger.com/feeds/latest/\n",
    "http://feeds.gawker.com/gizmodo/full\n",
    "http://we-make-money-not-art.com/feed/\n",
    "http://www.wired.com/rss/index.xml\n",
    "https://wonkette.com/feed'''.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists('feed.pickle'):\n",
    "    # The number of blogs each word appeared.\n",
    "    appeared_counts = {}\n",
    "\n",
    "    # The word count for each blog.\n",
    "    word_counts = {}\n",
    "\n",
    "    for url in feedlist:\n",
    "        try:\n",
    "            title, wc = get_word_counts(url)\n",
    "            word_counts[title] = wc\n",
    "            for word, count in wc.items():\n",
    "                appeared_counts.setdefault(word, 0)\n",
    "                if count > 1:\n",
    "                    appeared_counts[word] += 1\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 2\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "try:\n",
    "    with open('feed.pickle', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        print('loaded', len(data))\n",
    "        word_counts = data['word_counts']\n",
    "        appeared_counts = data['appeared_counts']\n",
    "except EOFError as e:\n",
    "    with open('feed.pickle', 'wb') as f:\n",
    "        data = {'appeared_counts': appeared_counts,\n",
    "                    'word_counts': word_counts}\n",
    "        pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist = []\n",
    "for w, bc in appeared_counts.items():\n",
    "    frac = float(bc) / len(feedlist)\n",
    "    # Skip the ones that are less than 10% and more than 50%.\n",
    "    if frac > 0.1 and frac < 0.5: wordlist.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('blogdata.txt', 'w') as f:\n",
    "    # First line is the header.\n",
    "    f.write('Blog')\n",
    "    for word in wordlist: f.write(f'\\t{word}')\n",
    "    f.write('\\n')\n",
    "    \n",
    "    # Subsequent lines are the body.\n",
    "    for blog, wc in word_counts.items():\n",
    "        if blog.strip() == '':\n",
    "            continue\n",
    "        f.write(blog)\n",
    "        for word in wordlist:\n",
    "            if word in wc: f.write(f'\\t{wc[word]}')\n",
    "            else: f.write(f'\\t{0}')\n",
    "        f.write('\\n')\n",
    "True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    lines = [line for line in open(filename)]\n",
    "    \n",
    "    # First line is the column title.\n",
    "    cols = lines[0].strip().split('\\t')[1:]\n",
    "    rows = []\n",
    "    data = []\n",
    "    for line in lines[1:]:\n",
    "        p = line.strip().split('\\t')\n",
    "        title = p[0]\n",
    "        if title.strip() == '': continue\n",
    "        \n",
    "        # First column in each row is the row name.\n",
    "        rows.append(p[0])\n",
    "        \n",
    "        # The data for this row is the remainder of the row.\n",
    "        data.append([float(x) for x in p[1:]])\n",
    "    return rows, cols, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols, data = read_file('blogdata.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "def pearsonr(v1, v2):\n",
    "    \"\"\"Pearson correlation determines how similar two items are.\"\"\"\n",
    "    \n",
    "    n = min(len(v1), len(v2))\n",
    "    v1, v2 = v1[:n], v2[:n]\n",
    "    # Simple sums.\n",
    "    sum1 = sum(v1)\n",
    "    sum2 = sum(v2)\n",
    "    \n",
    "    \n",
    "    # Sum of squares.\n",
    "    sum1_square = sum(pow(v, 2) for v in v1)\n",
    "    sum2_square = sum(pow(v, 2) for v in v2)\n",
    "    \n",
    "    # Sum of products.\n",
    "    sum_products = sum([v1[i] * v2[i] for i in range(n)])\n",
    "    \n",
    "    # Calculate the pearson score.\n",
    "    num = sum_products - (sum1 * sum2 / n)\n",
    "    den = sqrt((sum1_square - pow(sum1, 2) / n) * (sum2_square - pow(sum2, 2) / n))\n",
    "    if den == 0: return 0\n",
    "    return num / den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bicluster:\n",
    "    def __init__(self, vec, left = None, right = None, distance = 0.0, id = None):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.vec = vec\n",
    "        self.distance = distance\n",
    "        self.id = id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hcluster(rows, distance = pearsonr):\n",
    "    # Distances is the cache of the distance calculation.\n",
    "    distances = {}\n",
    "    current_cluster_id = -1\n",
    "    \n",
    "    # Clusters are initially just the rows.\n",
    "    cluster = [bicluster(rows[i], id = i) for i in range(len(rows))]\n",
    "    \n",
    "    while len(cluster) > 1:\n",
    "        lowest_pair = (0, 1)\n",
    "        closest = distance(cluster[0].vec, cluster[1].vec)\n",
    "        \n",
    "        # Loop through every pair looking for the smallest distance.\n",
    "        for i in range(len(cluster)):\n",
    "            for j in range(i + 1, len(cluster)):\n",
    "                key = (cluster[i].id, cluster[j].id)\n",
    "                if key not in distances:\n",
    "                    distances[key] = distance(cluster[i].vec, cluster[j].vec)\n",
    "                d = distances[key]\n",
    "                if d < closest:\n",
    "                    closest = d\n",
    "                    lowest_pair = (i, j)\n",
    "        (x, y) = lowest_pair\n",
    "\n",
    "        # Calculate the average of the two clusters.\n",
    "        merge_vectors = [(cluster[x].vec[i] + cluster[y].vec[i])/2.0 \n",
    "                         for i in range(len(cluster[0].vec))]\n",
    "        \n",
    "        # Create the new cluster.\n",
    "        new_cluster = bicluster(merge_vectors, \n",
    "                                left=cluster[x],\n",
    "                                right=cluster[y],\n",
    "                                distance=closest,\n",
    "                                id=current_cluster_id)\n",
    "        \n",
    "        # Cluster ids that weren't in the original set are negative.\n",
    "        current_cluster_id -= 1\n",
    "        del cluster[y]\n",
    "        del cluster[x]\n",
    "        cluster.append(new_cluster)\n",
    "    return cluster[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "blognames, words, data = read_file('blogdata.txt')\n",
    "cluster = hcluster(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cluster(cluster, labels = None, n = 0):\n",
    "    # Indent to make a hierachy layout.\n",
    "    for i in range(n): print(' ', end='')\n",
    "    if cluster.id < 0:\n",
    "        # Negative id means that this is branch.\n",
    "        print('-')\n",
    "    else:\n",
    "        # Positive id means that this is an endpoint.\n",
    "        if labels == None: print(cluster.id),\n",
    "        else: print(labels[cluster.id]),\n",
    "    \n",
    "    # Now print the right and left branches.\n",
    "    if cluster.left != None: print_cluster(cluster.left, labels = labels, n = n + 1)\n",
    "    if cluster.right != None: print_cluster(cluster.right, labels = labels, n= n + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      " -\n",
      "  -\n",
      "   -\n",
      "    The Official Google Blog\n",
      "    -\n",
      "     Neil Gaiman's Journal\n",
      "     -\n",
      "      COOL HUNTING\n",
      "      -\n",
      "       Schneier on Security\n",
      "       Talking Points Memo\n",
      "   -\n",
      "    Wonkette\n",
      "    -\n",
      "     456 Berea Street\n",
      "     -\n",
      "      -\n",
      "       Seth Godin's Blog on marketing, tribes and respect\n",
      "       -\n",
      "        Power LinePower Line\n",
      "        -\n",
      "         PaulStamatiou.com - Technology, Design and Photography\n",
      "         Engadget RSS Feed\n",
      "      -\n",
      "       Matt Cutts: Gadgets, Google, and SEO\n",
      "       ReadWrite\n",
      "  -\n",
      "   -\n",
      "    Slashdot\n",
      "    Derek Powazek\n",
      "   -\n",
      "    -\n",
      "     Eschaton\n",
      "     -\n",
      "      Mashable\n",
      "      -\n",
      "       Google Blogoscoped\n",
      "       -\n",
      "        Captain's Quarters\n",
      "        -\n",
      "         -\n",
      "          Autoblog\n",
      "          Lifehack - Feed\n",
      "         -\n",
      "          O'Reilly Radar\n",
      "          Tech\n",
      "    -\n",
      "     The Viral Garden\n",
      "     -\n",
      "      -\n",
      "       The Write News\n",
      "       Sifry's Alerts - David Sifry\n",
      "      -\n",
      "       -\n",
      "        Instapundit\n",
      "        -\n",
      "         Google Operating System\n",
      "         BuzzMachine\n",
      "       -\n",
      "        The Dish\n",
      "        Wired\n",
      " -\n",
      "  -\n",
      "   -\n",
      "    -\n",
      "     WIL WHEATON dot NET\n",
      "     -\n",
      "      Gizmodo\n",
      "      Copyblogger\n",
      "    -\n",
      "     Lifehacker\n",
      "     -\n",
      "      NB Blog Feed\n",
      "      -\n",
      "       Joi Ito's Web\n",
      "       Gapingvoid\n",
      "   -\n",
      "    Techdirt.\n",
      "    -\n",
      "     -\n",
      "      John Battelle's Search Blog\n",
      "      Deadspin\n",
      "     -\n",
      "      -\n",
      "       -\n",
      "        ProBlogger\n",
      "        TechCrunch\n",
      "       -\n",
      "        Latest from Crooks and Liars\n",
      "        -\n",
      "         Gigaom\n",
      "         mezzoblue\n",
      "      -\n",
      "       kottke.org\n",
      "       -\n",
      "        Creating Passionate Users\n",
      "        -\n",
      "         ThinkProgress\n",
      "         Quick Online Tips\n",
      "  -\n",
      "   -\n",
      "    -\n",
      "     -\n",
      "      pharyngula\n",
      "      -\n",
      "       MichelleMalkin.com\n",
      "       Make: DIY Projects and Ideas for Makers\n",
      "     -\n",
      "      ShoeMoney\n",
      "      -\n",
      "       Gothamist\n",
      "       -\n",
      "        Signal v. Noise\n",
      "        ABC News: BLOTTER\n",
      "    -\n",
      "     Boing Boing\n",
      "     -\n",
      "      SimpleBits\n",
      "      -\n",
      "       Guy Kawasaki\n",
      "       -\n",
      "        Search Engine Roundtable\n",
      "        -\n",
      "         blog maverick\n",
      "         Kotaku\n",
      "   -\n",
      "    ScienceBlogs - Where the world discusses science\n",
      "    -\n",
      "     ongoing by Tim Bray\n",
      "     -\n",
      "      -\n",
      "       Latest Items from TreeHugger\n",
      "       -\n",
      "        RSS feed – Search Engine Watch\n",
      "        TMZ.com\n",
      "      -\n",
      "       Joel on Software\n",
      "       We Make Money Not Art\n"
     ]
    }
   ],
   "source": [
    "print_cluster(cluster, blognames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
