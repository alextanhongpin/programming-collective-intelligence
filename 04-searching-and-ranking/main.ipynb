{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://resources.oreilly.com/examples/9780596529321/tree/master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping a Page\n",
    "\n",
    "We can scrape HTML pages using urllib3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(200,\n",
       " b'<!DOCTYPE html>\\n<html class=\"client-nojs\" lang=\"en\" dir=\"ltr\">\\n<head>\\n<meta charset=\"UTF-8\"/>\\n<title>Programming language - Wikipedia</title>\\n<script>document.documentElement.className=document.documentElement.className.replace(/(^|\\\\s)client-nojs(\\\\s|$)/,\"$1client-js$2\");RLCONF={\"wgCanonicalNamespace\":\"\",\"wgCanonicalSpecialPageName\":!1,\"wgNamespaceNumber\":0,\"wgPageName\":\"Programming_language\",\"wgTitle\":\"Programming language\",\"wgCurRevisionId\":901931155,\"wgRevisionId\":901931155,\"wgArticleId\":23015')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib3\n",
    "\n",
    "http = urllib3.PoolManager()\n",
    "url = 'https://en.wikipedia.org/wiki/Programming_language'\n",
    "# ! The example link is deprecated.\n",
    "# 'http://kiwitobes.com/wiki/Programming_language.html'\n",
    "r = http.request('GET', url)\n",
    "r.status, r.data[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing HTML\n",
    "\n",
    "Once we obtained the HTML page, use BeautifulSoup to parse the page and get the text as well as the links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\n\\n\\n\\n\\nProgramming language\\n\\nFrom Wikipedia, the'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Parse the HTML content of the page.\n",
    "soup = BeautifulSoup(r.data, 'html.parser')\n",
    "body = soup.body\n",
    "body.get_text()[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Text\n",
    "\n",
    "Lowercase all text and split them by white spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def separate_words(text):\n",
    "    \"\"\"Separate the words by non-whitespace character.\"\"\"\n",
    "    splitter = re.compile('\\W+')\n",
    "    return [s.lower()                     # Lowercase the words\n",
    "            for s in splitter.split(text) # for each splitted text\n",
    "            if s != '' and len(s) > 3]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['programming',\n",
       " 'language',\n",
       " 'from',\n",
       " 'wikipedia',\n",
       " 'free',\n",
       " 'encyclopedia',\n",
       " 'this',\n",
       " 'latest',\n",
       " 'accepted',\n",
       " 'revision',\n",
       " 'reviewed',\n",
       " 'june',\n",
       " '2019',\n",
       " 'jump',\n",
       " 'navigation',\n",
       " 'jump',\n",
       " 'search',\n",
       " 'language',\n",
       " 'designed',\n",
       " 'communicate']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "separate_words(body.get_text())[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get links\n",
    "\n",
    "To scrape the website recursively, we get all the links and repeat the process above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a id=\"top\"></a>,\n",
       " <a href=\"/wiki/Wikipedia:Protection_policy#pending\" title=\"All edits by unregistered and new users are subject to review prior to becoming visible to unregistered users\"><img alt=\"Page protected with pending changes\" data-file-height=\"512\" data-file-width=\"512\" decoding=\"async\" height=\"20\" src=\"//upload.wikimedia.org/wikipedia/en/thumb/b/b7/Pending-protection-shackle.svg/20px-Pending-protection-shackle.svg.png\" srcset=\"//upload.wikimedia.org/wikipedia/en/thumb/b/b7/Pending-protection-shackle.svg/30px-Pending-protection-shackle.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/b/b7/Pending-protection-shackle.svg/40px-Pending-protection-shackle.svg.png 2x\" width=\"20\"/></a>,\n",
       " <a href=\"/wiki/Wikipedia:Pending_changes\" title=\"Wikipedia:Pending changes\">latest accepted revision</a>,\n",
       " <a class=\"external text\" href=\"//en.wikipedia.org/w/index.php?title=Special:Log&amp;type=review&amp;page=Programming_language\">reviewed</a>,\n",
       " <a class=\"mw-jump-link\" href=\"#mw-head\">Jump to navigation</a>,\n",
       " <a class=\"mw-jump-link\" href=\"#p-search\">Jump to search</a>,\n",
       " <a class=\"image\" href=\"/wiki/File:C_Hello_World_Program.png\"><img alt=\"\" class=\"thumbimage\" data-file-height=\"655\" data-file-width=\"789\" decoding=\"async\" height=\"334\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/3/39/C_Hello_World_Program.png/402px-C_Hello_World_Program.png\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/3/39/C_Hello_World_Program.png/603px-C_Hello_World_Program.png 1.5x, //upload.wikimedia.org/wikipedia/commons/3/39/C_Hello_World_Program.png 2x\" width=\"402\"/></a>,\n",
       " <a class=\"internal\" href=\"/wiki/File:C_Hello_World_Program.png\" title=\"Enlarge\"></a>,\n",
       " <a href=\"/wiki/Source_code\" title=\"Source code\">source code</a>,\n",
       " <a href=\"/wiki/C_(programming_language)\" title=\"C (programming language)\">C programming language</a>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links = body.find_all('a')\n",
    "links[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rebuilding the links\n",
    "\n",
    "Note that the links are relative - we need to build back the full url path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://uz.wikipedia.org/wiki/Programmalash_tili',\n",
       " 'https://en.wikipedia.org/wiki/Library_(computing)',\n",
       " 'https://hsb.wikipedia.org/wiki/Program%C4%9Browanske_r%C4%9B%C4%8De',\n",
       " 'https://en.wikipedia.org/wiki/Undecidable_problem',\n",
       " 'https://en.wikipedia.org/wiki/Michael_Sipser',\n",
       " 'http://www.computerweekly.com/Articles/2007/09/11/226631/sslcomputer-weekly-it-salary-survey-finance-boom-drives-it-job.htm',\n",
       " 'https://en.wikipedia.org/wiki/Troff',\n",
       " 'http://www.apl.jhu.edu/~hall/Lisp-Notes/Macros.html',\n",
       " 'https://www.ibm.com/developerworks/library/os-erlang1/index.html',\n",
       " 'https://en.wikipedia.org/wiki/Formal_language']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.parse import urljoin\n",
    "\n",
    "fullurls = set()\n",
    "for link in links:\n",
    "    fullurl = urljoin(url, link.get('href'))\n",
    "    fullurl = fullurl.split('#')[0] # Remove location portion from the URL.\n",
    "    fullurl = fullurl.split('?')[0] # Remove querystring portion from the URL.\n",
    "    fullurls.add(fullurl)\n",
    "    \n",
    "list(fullurls)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Prepare the english stopwords.\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib3\n",
    "from bs4 import BeautifulSoup # pip3 install beautifulsoup4\n",
    "from urllib.parse import urljoin\n",
    "import sqlite3\n",
    "\n",
    "http = urllib3.PoolManager()\n",
    "url = 'https://en.wikipedia.org/wiki/Programming_language'\n",
    "\n",
    "class Crawler:\n",
    "    def __init__(self, db):\n",
    "        \"\"\"Initialize the crawler with the name of database.\"\"\"\n",
    "        self.conn = sqlite3.connect(db)\n",
    "    \n",
    "    def __del__(self):\n",
    "        \"\"\"Terminates the database connection.\"\"\"\n",
    "        self.conn.close()\n",
    "    \n",
    "    def get_entry_id(self, table, field, value, create_new = True):\n",
    "        \"\"\"Get an entry_id from a table, or create it if it does not exist.\"\"\"\n",
    "        c = self.conn.cursor()\n",
    "        \n",
    "        # ! We can't use dynamic table name for sqlite3.\n",
    "        res = c.execute(f'select rowid from {table} where {field} = ?', (value,)).fetchone()\n",
    "        \n",
    "        # The entry exists, returns the rowid.\n",
    "        if res != None:\n",
    "            # Execute returns a single sequence, or None.\n",
    "            return res[0]\n",
    "\n",
    "        # Else, insert the entry.\n",
    "        r = c.execute(f'insert into {table} ({field}) values (?)', (value,))\n",
    "        self.conn.commit()\n",
    "        \n",
    "        # Get the inserted rowid.\n",
    "        return r.lastrowid\n",
    "\n",
    "    def add_to_index(self, url, soup):\n",
    "        \"\"\"Index an individual page.\"\"\"\n",
    "        # Skip if the page has been indexed.\n",
    "        if self.is_indexed(url): return\n",
    "        \n",
    "        print(f'indexing {url}')\n",
    "        \n",
    "        # Get the individual words.\n",
    "        words = self.separate_words(soup.get_text())\n",
    "        \n",
    "        # Get the URL id which matches the current url in the database.\n",
    "        url_id = self.get_entry_id('urllist', 'url', url)\n",
    "        \n",
    "        # Link each word to this url.\n",
    "        for i, word in enumerate(words):\n",
    "            word_id = self.get_entry_id('wordlist', 'word', word)\n",
    "            stmt = 'insert into wordlocation(url_id, word_id, location) values (?, ?, ?)'\n",
    "            self.conn.execute(stmt, (url_id, \n",
    "                                     word_id, \n",
    "                                     i))\n",
    "\n",
    "    def separate_words(self, text):\n",
    "        \"\"\"Separate the words by non-whitespace character.\"\"\"\n",
    "        splitter = re.compile('\\W+')\n",
    "        return [s.lower()                     # Lowercase the words\n",
    "                for s in splitter.split(text) # for each splitted text\n",
    "                if len(s) > 3                 # at least 3 characters\n",
    "                and s not in stopwords]    # and not in the stopwords list.\n",
    "    \n",
    "    # Return true if this url is already indexed.\n",
    "    def is_indexed(self, url):\n",
    "        c = self.conn.cursor()\n",
    "        res = c.execute('select url from urllist where url = ?', (url,)).fetchone()\n",
    "        if res != None:\n",
    "            url = res[0]\n",
    "            # Check if it has actually been crawled.\n",
    "            res = c.execute('select * from wordlocation where url_id = ?', (url,)).fetchone()\n",
    "            if res != None: return True\n",
    "        return False\n",
    "    \n",
    "    # Add a link between pages.\n",
    "    def add_link_ref(self, url_from, url_to, link_text):\n",
    "        words = self.separate_words(link_text)\n",
    "        from_id = self.get_entry_id('urllist', 'url', url_from)\n",
    "        to_id = self.get_entry_id('urllist', 'url', url_to)\n",
    "        if from_id == to_id: return\n",
    "        \n",
    "        c = self.conn.cursor()\n",
    "        res = c.execute('insert into link(from_id,to_id) values (?, ?)', (from_id, to_id))\n",
    "        link_id = res.lastrowid\n",
    "        for word in words:\n",
    "            if word in stopwords: continue\n",
    "            word_id = self.get_entry_id('wordlist', 'word', word)\n",
    "            c.execute('insert into linkwords(link_id, word_id) values (?, ?)', (link_id, word_id))\n",
    "        self.conn.commit()\n",
    "\n",
    "    # Starting with a list of pages, do a breadth first search to the given depth,\n",
    "    # indexing pages as we go.\n",
    "    def crawl(self, pages, depth = 2):\n",
    "        # count = 0\n",
    "        # TODO: Implement max count so that the page doesn't crawl indefinitely.\n",
    "        # max_count = 1000\n",
    "        visited = set()\n",
    "        for i in range(depth):\n",
    "            new_pages = set()\n",
    "            for page in pages:\n",
    "                \n",
    "                if page in visited: continue # Skip visited page.\n",
    "                visited.add(page)            # Set page as visited.\n",
    "\n",
    "                try:\n",
    "                    r = http.request('GET', page)\n",
    "                    # r.status, r.data \n",
    "                    print(f'loaded page {page} {r.status}')\n",
    "                except:\n",
    "                    print(f'could not open page {page}')\n",
    "                    continue\n",
    "                \n",
    "                # Parse the HTML content of the page.\n",
    "                soup = BeautifulSoup(r.data, 'html.parser')\n",
    "                \n",
    "                # Index the HTML page. Take only the body.\n",
    "                self.add_to_index(page, soup.body)\n",
    "                \n",
    "                # Find all the href links in the page.\n",
    "                links = soup.body.find_all('a')\n",
    "                \n",
    "                # For each link, rebuild the full URL based on the base URL.\n",
    "                for link in links:\n",
    "                    url = urljoin(page, link.get('href'))\n",
    "                    url = url.split('#')[0] # Remove location portion from the URL.\n",
    "                    url = url.split('?')[0] # Remove querystring portion from the URL.\n",
    "                    if url in visited: continue # Skip if visited.\n",
    "\n",
    "                    # If the link starts with http (presumably a valid link) and it's not yet indexed...\n",
    "                    if url[0:4] == 'http' and not self.is_indexed(url):\n",
    "                        new_pages.add(url) # Add to the list of URLs to scrape.\n",
    "                    link_text = link.get_text() # Get the link text without the tags.\n",
    "                    \n",
    "                    # Add the reference from the link to the text.\n",
    "                    self.add_link_ref(page, url, link_text)\n",
    "                self.conn.commit()\n",
    "            pages = new_pages\n",
    "                \n",
    "    # Create the database tables.\n",
    "    def create_index_tables(self):\n",
    "        self.conn.execute('create table if not exists urllist(url)')\n",
    "        self.conn.execute('create table if not exists wordlist(word)')\n",
    "        self.conn.execute('create table if not exists wordlocation(url_id, word_id, location)')\n",
    "        self.conn.execute('create table if not exists link(from_id integer, to_id integer)')\n",
    "        self.conn.execute('create table if not exists linkwords(word_id, link_id)')\n",
    "        self.conn.execute('create index if not exists wordidx on wordlist(word)')\n",
    "        self.conn.execute('create index if not exists urlidx on urllist(url)')\n",
    "        self.conn.execute('create index if not exists wordurlidx on wordlocation(word_id)')\n",
    "        self.conn.execute('create index if not exists urltoidx on link(to_id)')\n",
    "        self.conn.execute('create index if not exists urlfromidx on link(from_id)')\n",
    "        self.conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler = Crawler('searchindex.db')\n",
    "crawler.create_index_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%skip` not found.\n"
     ]
    }
   ],
   "source": [
    "%skip\n",
    "crawler.crawl([\n",
    "    'https://en.wikipedia.org/wiki/Programming_language',\n",
    "    'https://en.wikipedia.org/wiki/Categorical_list_of_programming_languages.html',\n",
    "    'https://en.wikipedia.org/wiki/Functional_programming'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1798,)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [row for row in crawler.conn.execute('select rowid from wordlocation where word_id = 1')][:10]\n",
    "crawler.conn.execute('select count(rowid) from wordlocation where word_id = 1').fetchone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Searcher:\n",
    "    def __init__(self, db):\n",
    "        self.conn = sqlite3.connect(db)\n",
    "    \n",
    "    def __del__(self):\n",
    "        self.conn.close()\n",
    "\n",
    "    def get_match_rows(self, q):\n",
    "        # Strings to build the query.\n",
    "        fieldlist = 'w0.url_id'\n",
    "        tablelist = ''\n",
    "        clauselist = ''\n",
    "        wordids = []\n",
    "        \n",
    "        # Split the words by spaces.\n",
    "        words = q.split(' ')\n",
    "        tablenumber = 0\n",
    "        \n",
    "        for word in words:\n",
    "            # Get the word id.\n",
    "            c = self.conn.cursor()\n",
    "            wordrow = c.execute('select rowid from wordlist where word=?', (word,)).fetchone()\n",
    "            if wordrow != None:\n",
    "                wordid = wordrow[0]\n",
    "                wordids.append(wordid)\n",
    "                if tablenumber > 0:\n",
    "                    tablelist += ','\n",
    "                    clauselist += ' and '\n",
    "                    clauselist += f'w{tablenumber-1}.url_id=w{tablenumber}.url_id and '\n",
    "                fieldlist += f',w{tablenumber}.location'\n",
    "                tablelist += f'wordlocation w{tablenumber}'\n",
    "                clauselist += f'w{tablenumber}.word_id = {wordid}'\n",
    "                tablenumber += 1\n",
    "        # Create the query from the separate parts.\n",
    "        fullquery = f'select {fieldlist} from {tablelist} where {clauselist}'\n",
    "        print(fullquery)\n",
    "        c = self.conn.cursor()\n",
    "        res = c.execute(fullquery)\n",
    "        rows = [row for row in res]\n",
    "        return rows, wordids\n",
    "    \n",
    "    def get_scored_list(self, rows, word_ids):\n",
    "        total_scores = dict([(row[0], 0) for row in rows])\n",
    "        \n",
    "        # Put the scoring function here.\n",
    "        # weights = []\n",
    "        weights = [(1.0, self.frequency_score(rows)),\n",
    "                   (1.5, self.location_score(rows)),\n",
    "                   (1.0, self.page_rank_score(rows)),\n",
    "                   (1.0, self.link_text_score(rows, word_ids))]\n",
    "        \n",
    "        for (weight, scores) in weights:\n",
    "            for url in total_scores:\n",
    "                total_scores[url] += weight * scores[url]\n",
    "        \n",
    "        return total_scores\n",
    "\n",
    "    def get_url_name(self, id):\n",
    "        return self.conn.cursor().execute('select url from urllist where rowid = ?', (id,)).fetchone()[0]\n",
    "    \n",
    "    def query(self, q, n=10):\n",
    "        rows, word_ids = self.get_match_rows(q)\n",
    "        scores = self.get_scored_list(rows, word_ids)\n",
    "        ranked_scores = sorted([(score, url) \n",
    "                                for (url, score) \n",
    "                                in scores.items()], \n",
    "                               reverse=True)\n",
    "\n",
    "        for (score, url_id) in ranked_scores[0:n]:\n",
    "            print(f'{score}\\t{self.get_url_name(url_id)}')\n",
    "    \n",
    "    def normalize_scores(self, scores, small_is_better=False):\n",
    "        \"\"\"\n",
    "        Sometimes a smaller score is better, and vice versa. The normalization\n",
    "        function will take a dictionary of IDs and scores and return a new dictionary with the same IDs,\n",
    "        but with score between 0 and 1. Each score is scaled according to how close it is to the best \n",
    "        result, which will always have a score of 1.\n",
    "        \"\"\"\n",
    "        vsmall = 0.00001 # Avoid division by zero errors.\n",
    "        if small_is_better:\n",
    "            min_score = min(scores.values())\n",
    "            return dict([(u, float(min_score)/max(vsmall,1))\n",
    "                        for (u, l) in scores.items()])\n",
    "        else:\n",
    "            max_score = max(scores.values())\n",
    "            if max_score == 0: max_score = vsmall\n",
    "            return dict([(u, float(c) / max_score) for (u, c) in scores.items()])\n",
    "    \n",
    "    def frequency_score(self, rows):\n",
    "        \"\"\"\n",
    "        The word frequency scores a page based on how many times the words in the query appear on that page.\n",
    "        \"\"\"\n",
    "        counts = dict([(row[0], 0) for row in rows])\n",
    "        \n",
    "        # Create a dictionary with an entry for each unique url id, and count how many times the item appears.\n",
    "        for row in rows: counts[row[0]] += 1\n",
    "            \n",
    "        # Normalize the scores, in this case, bigger is better (occur more frequently).\n",
    "        return self.normalize_scores(counts)\n",
    "\n",
    "    def location_score(self, rows):\n",
    "        \"\"\"\n",
    "        Score the page based on the search term location in the page. \n",
    "        If a page is relevant to the search term, it will appear closer to the top.\n",
    "        The search engine can score results higher if the query term appears early\n",
    "        in the document.\"\"\"\n",
    "        locations = dict([(row[0], 1000000) for row in rows])\n",
    "        for row in rows:\n",
    "            loc = sum(row[1:])\n",
    "            if loc < locations[row[0]]: locations[row[0]] = loc\n",
    "        # The lowest location score (closes to the start) will get a score of 1.\n",
    "        return self.normalize_scores(locations, small_is_better=True)\n",
    "\n",
    "    def distance_score(self, rows):\n",
    "        \"\"\"\n",
    "        When a query contains multiple words, \n",
    "        it is often useful to seek results in which the words in \n",
    "        the query are close to each other in the page.\n",
    "        \"\"\"\n",
    "        # If there are only one word, everyone wins!\n",
    "        if len(rows[0]) <= 2: return dict([(row[0], 1.0) for row in rows])\n",
    "        \n",
    "        # Initialize the dictionary with large values.\n",
    "        min_distance = dict([(row[0], 1000000) for row in rows])\n",
    "        for row in rows:\n",
    "            dist = sum([abs(row[i] - row[i-1])\n",
    "                       for i in range(2, len(row))])\n",
    "            if dist < min_distance[row[0]]: min_distance[row[0]] = dist\n",
    "        # The smaller the distance, means the more similar the results are.\n",
    "        return self.normalize_scores(min_distance, True)\n",
    "\n",
    "    def inbound_link_score(self, rows):\n",
    "        \"\"\"\n",
    "        Count the inbound links on the page and use the total number of links as a metric for the page.\n",
    "        \"\"\"\n",
    "        unique_urls = set([row[0] for row in rows])\n",
    "        inbound_count = dict([(u, self.conn.cursor().execute('select count(*) from link where to_id = ?', (u,)).fetchone()[0]) for u in unique_urls])\n",
    "        return self.normalize_scores(inbound_count)\n",
    "    \n",
    "    def calculate_page_rank(self, iterations=20):\n",
    "        # Clear out the current page rank tables.\n",
    "        c = self.conn.cursor()\n",
    "        c.execute('drop table if exists pagerank')\n",
    "        c.execute('create table pagerank(url_id primary key, score)')\n",
    "        \n",
    "        # Initialize every query with a PageRank of 1.\n",
    "        c.execute('insert into pagerank select rowid, 1.0 from urllist')\n",
    "        self.conn.commit()\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            print(f'iteration {i}')\n",
    "            \n",
    "            for (url_id,) in c.execute('select rowid from urllist'):\n",
    "                pr = 0.15\n",
    "                \n",
    "                # Loop through all the pages that link to this one.\n",
    "                for (linker,) in c.execute(\n",
    "                'select distinct from_id from link where to_id = ?', (url_id,)):\n",
    "                    # Get the page rank of the linker.\n",
    "                    linkingpr = c.execute(\n",
    "                    'select score from pagerank where url_id = ?', (linker,)).fetchone()[0]\n",
    "                    \n",
    "                    # Get the total number of links from the linker.\n",
    "                    linking_count = c.execute(\n",
    "                    'select count(*) from link where from_id = ?', (linker,)).fetchone()[0]\n",
    "                    \n",
    "                    pr + 0.85 * (linkingpr / linking_count)\n",
    "                \n",
    "                c.execute(\n",
    "                'update pagerank set score = ? where url_id = ?', (pr, url_id))\n",
    "            self.conn.commit()\n",
    "    \n",
    "    def page_rank_score(self, rows):\n",
    "        c = self.conn.cursor()\n",
    "        page_ranks = dict([(row[0], c.execute('select score from pagerank where url_id = ?', (row[0],)).fetchone()[0]) for row in rows])\n",
    "        max_rank = max(page_ranks.values())\n",
    "        normalized_scores = dict([(u, float(1)/max_rank) for (u,l) in page_ranks.items()])\n",
    "        return normalized_scores\n",
    "    \n",
    "    def link_text_score(self, rows, word_ids):\n",
    "        \"\"\"\n",
    "        Score the page based on the text of the links to a page to decide how relevant the page is.\n",
    "        \"\"\"\n",
    "        link_scores = dict([(row[0],0) for row in rows])\n",
    "        c = self.conn.cursor()\n",
    "        for word_id in word_ids:\n",
    "            cur = c.execute('select link.from_id, link.to_id from linkwords, link where word_id = ? and linkwords.link_id = link.rowid', (word_id,))\n",
    "            \n",
    "            for (from_id, to_id) in cur:\n",
    "                if to_id in link_scores:\n",
    "                    pr = c.execute(\"\"\"\n",
    "                    select score \n",
    "                    from pagerank \n",
    "                    where url_id = ?\"\"\", (from_id,)).fetchone()[0]\n",
    "                    link_scores[to_id] += pr\n",
    "        max_score = max(link_scores.values())\n",
    "        vsmall = 0.00001 # Avoid division by zero errors.\n",
    "        if max_score == 0: max_score = vsmall\n",
    "        normalized_scores = dict([(u, float(l)/max_score) for (u,l) in link_scores.items()])\n",
    "        return normalized_scores\n",
    "\n",
    "    # import nn\n",
    "    # net = nn.searchnet(\"nn.db\")\n",
    "#     def nn_score(self, rows, wordids):\n",
    "#         # Get unique url ids as an ordered list.\n",
    "#         urlids = [urlid for urlif in set([row[0] for row in rows])]\n",
    "#         nnres = net.getresult(wordids, urlids)\n",
    "#         scores = dict([(urlids[i], nnres[i]) for i in range(len(urlids))])\n",
    "#         return self.normalize_scores(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select w0.url_id,w0.location,w1.location from wordlocation w0,wordlocation w1 where w0.word_id = 432 and w0.url_id=w1.url_id and w1.word_id = 1\n",
      "query.scores {1: 3.1166107382550337, 505: 2.505838926174497, 105: 4.5}\n",
      "4.5\thttps://en.wikipedia.org/wiki/Functional_programming\n",
      "3.1166107382550337\thttps://en.wikipedia.org/wiki/Programming_language\n",
      "2.505838926174497\thttps://en.wikipedia.org/wiki/Objective-C\n"
     ]
    }
   ],
   "source": [
    "engine = Searcher('searchindex.db')\n",
    "# engine.get_match_rows('functional programming')\n",
    "engine.query('functional programming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "iteration 10\n",
      "iteration 11\n",
      "iteration 12\n",
      "iteration 13\n",
      "iteration 14\n",
      "iteration 15\n",
      "iteration 16\n",
      "iteration 17\n",
      "iteration 18\n",
      "iteration 19\n"
     ]
    }
   ],
   "source": [
    "engine.calculate_page_rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select w0.url_id,w0.location,w1.location from wordlocation w0,wordlocation w1 where w0.word_id = 124 and w0.url_id=w1.url_id and w1.word_id = 1\n",
      "query.scores {1: 69.5, 505: 68.69885714285714, 105: 69.52176870748299}\n",
      "69.52176870748299\thttps://en.wikipedia.org/wiki/Functional_programming\n",
      "69.5\thttps://en.wikipedia.org/wiki/Programming_language\n",
      "68.69885714285714\thttps://en.wikipedia.org/wiki/Objective-C\n"
     ]
    }
   ],
   "source": [
    "engine.query('dynamic programming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[row for row in engine.conn.cursor().execute('select * from wordlist limit 10')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content-based Ranking\n",
    "\n",
    "- word frequency: the number of times the words in the query appear in the document can help determine how relevant the document is\n",
    "- document location: the main subject of a document will probably appear near the beginning of the document\n",
    "- word distance: if there are multiple words in the query, they should appear close together in the document\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
